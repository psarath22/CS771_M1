{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Emoticon Dataset**"
      ],
      "metadata": {
        "id": "xgHKYzuNNkhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1t_IorYDJVvf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import requests\n",
        "np.random.seed(0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Read emoticon dataset\n",
        "train_emoticon_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/train/train_emoticon.csv\")\n",
        "train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
        "train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
        "\n",
        "valid_emoticon_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/valid/valid_emoticon.csv\")\n",
        "valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()\n",
        "valid_emoticon_Y = valid_emoticon_df['label'].tolist()\n",
        "\n",
        "test_emoticon_X = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/test/test_emoticon.csv\")['input_emoticon'].tolist()\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "train_emoticon_X = np.array(train_emoticon_X)\n",
        "valid_emoticon_X = np.array(valid_emoticon_X)\n",
        "test_emoticon_X = np.array(test_emoticon_X)\n",
        "\n",
        "# Ensure labels are NumPy arrays\n",
        "train_emoticon_Y = np.array(train_emoticon_Y)\n",
        "valid_emoticon_Y = np.array(valid_emoticon_Y)\n",
        "\n",
        "# Concatenate all texts to get unique characters\n",
        "all_texts = ''.join(train_emoticon_X.flatten()) + ''.join(valid_emoticon_X.flatten()) + ''.join(test_emoticon_X.flatten())\n",
        "unique_characters = np.unique(list(all_texts))\n",
        "\n",
        "# Create character-to-index mapping\n",
        "char_to_index = {char: idx + 1 for idx, char in enumerate(unique_characters)}  # Index starts from 1 for embedding\n",
        "num_classes = len(unique_characters) + 1  # Include 0 for padding\n",
        "\n",
        "# Convert sequences to integer sequences\n",
        "def convert_to_sequences(sequences):\n",
        "    return [[char_to_index[char] for char in seq] for seq in sequences]\n",
        "\n",
        "# Convert to integer sequences\n",
        "train_emoticon_X = convert_to_sequences(train_emoticon_X)\n",
        "valid_emoticon_X = convert_to_sequences(valid_emoticon_X)\n",
        "test_emoticon_X = convert_to_sequences(test_emoticon_X)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_len = 13  # Fixed sequence length\n",
        "train_emoticon_X = pad_sequences(train_emoticon_X, maxlen=max_len, padding='post')\n",
        "valid_emoticon_X = pad_sequences(valid_emoticon_X, maxlen=max_len, padding='post')\n",
        "test_emoticon_X = pad_sequences(test_emoticon_X, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model for feature extraction\n",
        "embedding_dimension = 12 # Dimension of the embedding\n",
        "# Define and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_classes, output_dim=embedding_dimension))  # Added input_length\n",
        "model.add(Flatten())\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_emoticon_X, train_emoticon_Y, epochs=10, batch_size=32, validation_data=(valid_emoticon_X, valid_emoticon_Y))\n",
        "\n",
        "\n",
        "# Predictions and calculate F1 score and confusion matrix\n",
        "valid_predictions_emoticon = (model.predict(valid_emoticon_X) > 0.5).astype(\"int32\")\n",
        "train_predictions_emoticon = (model.predict(train_emoticon_X) > 0.5).astype(\"int32\")\n",
        "test_predictions_emoticon = (model.predict(test_emoticon_X) > 0.5).astype(\"int32\")\n",
        "\n",
        "valid_predictions_emoticon = valid_predictions_emoticon.flatten()\n",
        "train_predictions_emoticon = train_predictions_emoticon.flatten()\n",
        "test_predictions_emoticon = test_predictions_emoticon.flatten()\n",
        "\n",
        "f1 = f1_score(valid_emoticon_Y, valid_predictions_emoticon)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "valid_accuracy_emoticon = accuracy_score(valid_emoticon_Y, valid_predictions_emoticon)\n",
        "# Calculate training accuracy\n",
        "train_accuracy_emoticon = accuracy_score(train_emoticon_Y, train_predictions_emoticon)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(f\"Training Accuracy for emoticon: {train_accuracy_emoticon:.4f}\")\n",
        "print(f\"Validation Accuracy for emoticon: {valid_accuracy_emoticon:.4f}\")\n",
        "print(f\"F1 Score for emoticon: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "9z1gv_9sJqHM",
        "outputId": "c7a302a9-c36d-4723-9db5-ea5e1f5cae18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5227 - loss: 0.6902 - val_accuracy: 0.8937 - val_loss: 0.5502\n",
            "Epoch 2/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9270 - loss: 0.3790 - val_accuracy: 0.9632 - val_loss: 0.1451\n",
            "Epoch 3/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9724 - loss: 0.1206 - val_accuracy: 0.9714 - val_loss: 0.0974\n",
            "Epoch 4/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9824 - loss: 0.0757 - val_accuracy: 0.9734 - val_loss: 0.0756\n",
            "Epoch 5/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0637 - val_accuracy: 0.9796 - val_loss: 0.0652\n",
            "Epoch 6/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.0497 - val_accuracy: 0.9816 - val_loss: 0.0552\n",
            "Epoch 7/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0410 - val_accuracy: 0.9816 - val_loss: 0.0501\n",
            "Epoch 8/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9915 - loss: 0.0372 - val_accuracy: 0.9816 - val_loss: 0.0435\n",
            "Epoch 9/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0279 - val_accuracy: 0.9857 - val_loss: 0.0439\n",
            "Epoch 10/10\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0218 - val_accuracy: 0.9796 - val_loss: 0.0466\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m)              │           \u001b[38;5;34m2,724\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │           \u001b[38;5;34m3,768\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m25\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,724</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,768</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,553\u001b[0m (76.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,553</span> (76.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,517\u001b[0m (25.46 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,517</span> (25.46 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m13,036\u001b[0m (50.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,036</span> (50.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy for emoticon: 0.9982\n",
            "Validation Accuracy for emoticon: 0.9796\n",
            "F1 Score for emoticon: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_predictions_emoticon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCUHGYmFJqy7",
        "outputId": "9ef1ef2f-3ca8-4c03-b768-763ebbf5cbfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 ... 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Features Dataset**"
      ],
      "metadata": {
        "id": "SA9jIOYFNq7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the raw .npz file\n",
        "url_train = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/train/train_feature.npz\"\n",
        "url_valid = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/valid/valid_feature.npz\"\n",
        "url_test = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/test/test_feature.npz\"\n",
        "# Download the file\n",
        "response1 = requests.get(url_train)\n",
        "response2= requests.get(url_valid)\n",
        "response3 = requests.get(url_test)\n",
        "\n",
        "# Save the content to a file\n",
        "with open('train_feature.npz', 'wb') as f:\n",
        "    f.write(response1.content)\n",
        "    print(\"Downloading TrainData completed!\")\n",
        "\n",
        "# Save the content to a file\n",
        "with open('valid_feature.npz', 'wb') as f:\n",
        "    f.write(response2.content)\n",
        "    print(\"Downloading validData completed!\")\n",
        "\n",
        "# Save the content to a file\n",
        "with open('test_feature.npz', 'wb') as f:\n",
        "    f.write(response3.content)\n",
        "    print(\"Downloading TestData completed!\")\n",
        "\n",
        "\n",
        "# read feat dataset\n",
        "train_feat = np.load(\"train_feature.npz\", allow_pickle=True)\n",
        "train_feat_X = train_feat['features']\n",
        "train_feat_Y = train_feat['label']\n",
        "\n",
        "test_feat_X = np.load(\"test_feature.npz\", allow_pickle=True)['features']\n",
        "\n",
        "valid_feat = np.load(\"valid_feature.npz\", allow_pickle=True)\n",
        "valid_feat_X = valid_feat['features']\n",
        "valid_feat_Y = valid_feat['label']\n",
        "\n",
        "\n",
        "print(f\"Train dataset size: \")\n",
        "print(f\"train_feat_X: {len(train_feat_X)} train_feat_Y: {len(train_feat_Y)}\")\n",
        "\n",
        "print()\n",
        "print(\"Valid dataset size: \")\n",
        "print(f\"valid_feat_X: {len(valid_feat_X)} valid_feat_Y: {len(valid_feat_Y)}\")\n",
        "\n",
        "print()\n",
        "print(\"Test dataset size: \")\n",
        "print(f\"test_feat_X: {len(test_feat_X)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHcTEbE1L_Go",
        "outputId": "7f2d88d1-8a64-4c64-ddcf-6f8ec7640a24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TrainData completed!\n",
            "Downloading validData completed!\n",
            "Downloading TestData completed!\n",
            "Train dataset size: \n",
            "train_feat_X: 7080 train_feat_Y: 7080\n",
            "\n",
            "Valid dataset size: \n",
            "valid_feat_X: 489 valid_feat_Y: 489\n",
            "\n",
            "Test dataset size: \n",
            "test_feat_X: 2232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training and validation  dataset\n",
        "\n",
        "train_feat_X = np.array([feat for feat in train_feat_X])\n",
        "print(train_feat_X.shape)\n",
        "valid_feat_X = np.array([feat for feat in valid_feat_X])\n",
        "print(valid_feat_X.shape)\n",
        "test_feat_X = np.array([feat for feat in test_feat_X])\n",
        "print(test_feat_X.shape)\n",
        "\n",
        "train_feat_X = np.array([feat.flatten() for feat in train_feat_X])\n",
        "valid_feat_X = np.array([feat.flatten() for feat in valid_feat_X])\n",
        "test_feat_X = np.array([feat.flatten() for feat in test_feat_X])\n",
        "\n",
        "n_components = 150\n",
        "pca = PCA(n_components=n_components)\n",
        "train_feat_X = pca.fit_transform(train_feat_X)\n",
        "valid_feat_X = pca.transform(valid_feat_X)\n",
        "test_feat_X = pca.transform(test_feat_X)\n",
        "print(train_feat_X.shape)\n",
        "print(valid_feat_X.shape)\n",
        "print(test_feat_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGcaF6zjMP0R",
        "outputId": "7042b1ec-78cd-4b6b-a165-339d8f1726b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7080, 13, 768)\n",
            "(489, 13, 768)\n",
            "(2232, 13, 768)\n",
            "(7080, 150)\n",
            "(489, 150)\n",
            "(2232, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression classifier\n",
        "logreg_classifier = LogisticRegression(max_iter=1000)\n",
        "logreg_classifier.fit(train_feat_X, train_feat_Y)\n",
        "\n",
        "# Predict classes for validation data\n",
        "valid_predictions_feat = logreg_classifier.predict(valid_feat_X)\n",
        "train_predictions_feat = logreg_classifier.predict(train_feat_X)\n",
        "test_predictions_feat = logreg_classifier.predict(test_feat_X)\n",
        "\n",
        "valid_predictions_feat = valid_predictions_feat.flatten()\n",
        "train_predictions_feat = train_predictions_feat.flatten()\n",
        "test_predictions_feat = test_predictions_feat.flatten()\n",
        "\n",
        "# Calculate validation accuracy\n",
        "valid_accuracy_feat = accuracy_score(valid_feat_Y, valid_predictions_feat)\n",
        "print(f\"Validation Accuracy with Logistic Regression for features dataset: {valid_accuracy_feat}\")\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_accuracy_feat = accuracy_score(train_feat_Y, train_predictions_feat)\n",
        "print(f\"Train Accuracy with Logistic Regression for features dataset: {train_accuracy_feat}\")\n",
        "\n",
        "f1 = f1_score(valid_feat_Y, valid_predictions_feat, average='weighted')\n",
        "print(f\"f1 score with logistic Regression  for features dataset: {f1}\")\n",
        "\n",
        "# Calculate the number of trainable parameters\n",
        "n_weights = logreg_classifier.coef_.size  # Number of weights (features)\n",
        "n_intercepts = logreg_classifier.intercept_.size  # Number of intercepts (bias term)\n",
        "\n",
        "n_trainable_parameters = n_weights + n_intercepts\n",
        "print(f\"Number of trainable parameters  for features dataset: {n_trainable_parameters}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDB-2bN0MQWZ",
        "outputId": "349dfb6c-7ace-4098-f773-ce7acd1e7be6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy with Logistic Regression for features dataset: 0.9877300613496932\n",
            "Train Accuracy with Logistic Regression for features dataset: 0.9853107344632769\n",
            "f1 score with logistic Regression  for features dataset: 0.9877300613496932\n",
            "Number of trainable parameters  for features dataset: 151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_predictions_feat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8y7M24xN992",
        "outputId": "3017f4f8-5368-4d2a-b3be-e1a4ce411610"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 ... 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Sequence Dataset**"
      ],
      "metadata": {
        "id": "xb3HqeogOBkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read text sequence dataset\n",
        "train_seq_df = pd.read_csv('https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/train/train_text_seq.csv')\n",
        "train_seq_X = train_seq_df['input_str'].tolist()\n",
        "train_seq_Y = train_seq_df['label'].tolist()\n",
        "\n",
        "test_seq_X = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/test/test_text_seq.csv\")['input_str'].tolist()\n",
        "\n",
        "# read text sequence dataset\n",
        "valid_seq_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/valid/valid_text_seq.csv\")\n",
        "valid_seq_X = valid_seq_df['input_str'].tolist()\n",
        "valid_seq_Y = valid_seq_df['label'].tolist()\n",
        "\n",
        "print(f\"Train dataset size: \")\n",
        "print(f\"train_seq_X: {len(train_seq_X)} train_seq_Y: {len(train_seq_Y)}\")\n",
        "\n",
        "print()\n",
        "print(\"Valid dataset size: \")\n",
        "print(f\"valid_seq_X: {len(valid_seq_X)} valid_seq_Y: {len(valid_seq_Y)}\")\n",
        "\n",
        "print()\n",
        "print(\"Test dataset size: \")\n",
        "print(f\"test_seq_X: {len(test_seq_X)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIQtLRdrOAhE",
        "outputId": "da299044-6fdc-4184-c5ca-8d457a974b7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: \n",
            "train_seq_X: 7080 train_seq_Y: 7080\n",
            "\n",
            "Valid dataset size: \n",
            "valid_seq_X: 489 valid_seq_Y: 489\n",
            "\n",
            "Test dataset size: \n",
            "test_seq_X: 2232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list to NumPy array\n",
        "train_seq_X = np.array(train_seq_X)\n",
        "valid_seq_X = np.array(valid_seq_X)\n",
        "test_seq_X = np.array(test_seq_X)\n",
        "# Convert labels to numpy arrays\n",
        "train_seq_Y = np.array(train_seq_Y)\n",
        "valid_seq_Y = np.array(valid_seq_Y)\n",
        "\n",
        "print(type(train_seq_X))\n",
        "print(type(train_seq_X[0]))\n",
        "# Convert list of strings (2D array) to a single string by flattening the array and joining all elements\n",
        "all_characters = ''.join(train_seq_X.flatten())\n",
        "\n",
        "# Get the unique characters\n",
        "unique_characters = np.unique(list(all_characters))\n",
        "\n",
        "print(f\"Unique characters: {unique_characters}\")\n",
        "print(f\"Number of unique characters: {len(unique_characters)}\")\n",
        "print(np.unique(valid_seq_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7rxsU5lOVbo",
        "outputId": "15bbeb8b-4b7f-4a7f-e2f5-f4b54a497290"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.str_'>\n",
            "Unique characters: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
            "Number of unique characters: 10\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique characters for two digits\n",
        "unique_characters = [f\"{i:02d}\" for i in range(100)]  # ['00', '01', ..., '99']\n",
        "num_classes = len(unique_characters)  # 100 classes\n",
        "\n",
        "# Create a mapping from characters to indices\n",
        "char_to_index = {char: idx for idx, char in enumerate(unique_characters)}\n",
        "\n",
        "# Function to convert input seqs to a 49x100 matrix using overlapping pairs\n",
        "def one_hot_count_matrix(seq, max_len=50):\n",
        "    # Initialize a matrix of zeros\n",
        "    matrix = np.zeros((max_len - 1, num_classes))  # 49x100 matrix for overlapping pairs\n",
        "    # Extract overlapping pairs and update the matrix\n",
        "    for i in range(0, max_len - 1):  # Loop from seq[0:2] to seq[48:50]\n",
        "        char_pair = seq[i:i + 2]  # Get overlapping pairs of characters\n",
        "        if char_pair in char_to_index:\n",
        "            index = char_to_index[char_pair]\n",
        "            matrix[i, index] = 1  # One-hot encoding\n",
        "    return matrix.flatten()  # Flatten to a vector for SVM input\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_seq_X = np.array([one_hot_count_matrix(seq) for seq in train_seq_X])\n",
        "valid_seq_X = np.array([one_hot_count_matrix(seq) for seq in valid_seq_X])\n",
        "test_seq_X = np.array([one_hot_count_matrix(seq) for seq in test_seq_X])\n",
        "\n",
        "print(train_seq_X.shape)  # Expected shape: (N_train, 4900)\n",
        "print(valid_seq_X.shape)  # Expected shape: (N_valid, 4900)\n",
        "print(test_seq_X.shape) #Expected shape: (N_test, 4900)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLBS8-EwOYpp",
        "outputId": "456d77d5-4776-4cde-930f-435c42048cba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7080, 4900)\n",
            "(489, 4900)\n",
            "(2232, 4900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input to fit CNN (samples, timesteps, features)\n",
        "train_seq_X = train_seq_X.reshape((train_seq_X.shape[0], 49, 100))\n",
        "valid_seq_X = valid_seq_X.reshape((valid_seq_X.shape[0], 49, 100))\n",
        "test_seq_X = test_seq_X.reshape((test_seq_X.shape[0], 49, 100))\n",
        "\n",
        "print(f\"train_seq_X shape: {train_seq_X.shape}\")\n",
        "print(f\"valid_seq_X shape: {valid_seq_X.shape}\")\n",
        "print(f\"test_seq_X shape: {test_seq_X.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yof_mj14OdpJ",
        "outputId": "c17d1370-98f1-41de-d302-2c214b5367de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_seq_X shape: (7080, 49, 100)\n",
            "valid_seq_X shape: (489, 49, 100)\n",
            "test_seq_X shape: (2232, 49, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN model with fewer parameters\n",
        "model = Sequential()\n",
        "# Input layer\n",
        "model.add(Input(shape=(49, 100)))\n",
        "model.add(Conv1D(filters=16, kernel_size=2, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))  # Smaller dense layer\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_seq_X, train_seq_Y, epochs=30, batch_size=32, validation_data=(valid_seq_X, valid_seq_Y))\n",
        "\n",
        "# Predictions and calculate F1 score and confusion matrix\n",
        "valid_predictions_seq = (model.predict(valid_seq_X) > 0.5).astype(\"int32\")\n",
        "train_predictions_seq = (model.predict(train_seq_X) > 0.5).astype(\"int32\")\n",
        "test_predictions_seq = (model.predict(test_seq_X) > 0.5).astype(\"int32\")\n",
        "f1 = f1_score(valid_seq_Y, valid_predictions_seq)\n",
        "\n",
        "valid_predictions_seq = valid_predictions_seq.flatten()\n",
        "train_predictions_seq = train_predictions_seq.flatten()\n",
        "test_predictions_seq = test_predictions_seq.flatten()\n",
        "\n",
        "# Calculate validation accuracy\n",
        "valid_accuracy_seq = accuracy_score(valid_seq_Y, valid_predictions_seq)\n",
        "# Calculate training accuracy\n",
        "train_accuracy_seq = accuracy_score(train_seq_Y, train_predictions_seq)\n",
        "\n",
        "# Print accuracies and F1 score\n",
        "model.summary()\n",
        "\n",
        "print(f\"Training Accuracy for sequence data: {train_accuracy_seq:.4f}\")\n",
        "print(f\"Validation Accuracy for sequence data: {valid_accuracy_seq:.4f}\")\n",
        "print(f\"F1 Score for sequence data: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-XPTacCQOg5c",
        "outputId": "4aab30f9-7491-40f9-9941-6bc6d2cf2a6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5385 - loss: 0.6775 - val_accuracy: 0.7485 - val_loss: 0.5157\n",
            "Epoch 2/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7753 - loss: 0.4786 - val_accuracy: 0.8425 - val_loss: 0.3636\n",
            "Epoch 3/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8358 - loss: 0.3890 - val_accuracy: 0.8569 - val_loss: 0.3081\n",
            "Epoch 4/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8461 - loss: 0.3532 - val_accuracy: 0.8569 - val_loss: 0.3055\n",
            "Epoch 5/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8638 - loss: 0.3296 - val_accuracy: 0.8650 - val_loss: 0.2885\n",
            "Epoch 6/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8637 - loss: 0.3163 - val_accuracy: 0.8650 - val_loss: 0.2730\n",
            "Epoch 7/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8732 - loss: 0.2912 - val_accuracy: 0.8569 - val_loss: 0.2779\n",
            "Epoch 8/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8674 - loss: 0.3094 - val_accuracy: 0.8691 - val_loss: 0.2781\n",
            "Epoch 9/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8827 - loss: 0.2846 - val_accuracy: 0.8609 - val_loss: 0.2543\n",
            "Epoch 10/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8719 - loss: 0.2947 - val_accuracy: 0.8753 - val_loss: 0.2517\n",
            "Epoch 11/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8922 - loss: 0.2607 - val_accuracy: 0.8691 - val_loss: 0.2590\n",
            "Epoch 12/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8783 - loss: 0.2849 - val_accuracy: 0.8732 - val_loss: 0.2615\n",
            "Epoch 13/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8775 - loss: 0.2747 - val_accuracy: 0.8650 - val_loss: 0.2531\n",
            "Epoch 14/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8879 - loss: 0.2689 - val_accuracy: 0.8753 - val_loss: 0.2602\n",
            "Epoch 15/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8910 - loss: 0.2671 - val_accuracy: 0.8671 - val_loss: 0.2741\n",
            "Epoch 16/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8883 - loss: 0.2720 - val_accuracy: 0.8569 - val_loss: 0.2552\n",
            "Epoch 17/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8875 - loss: 0.2651 - val_accuracy: 0.8834 - val_loss: 0.2539\n",
            "Epoch 18/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8799 - loss: 0.2925 - val_accuracy: 0.8793 - val_loss: 0.2446\n",
            "Epoch 19/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8892 - loss: 0.2666 - val_accuracy: 0.8834 - val_loss: 0.2491\n",
            "Epoch 20/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8829 - loss: 0.2796 - val_accuracy: 0.8732 - val_loss: 0.2587\n",
            "Epoch 21/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8969 - loss: 0.2573 - val_accuracy: 0.8630 - val_loss: 0.2696\n",
            "Epoch 22/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8959 - loss: 0.2567 - val_accuracy: 0.8753 - val_loss: 0.2472\n",
            "Epoch 23/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8797 - loss: 0.2613 - val_accuracy: 0.8671 - val_loss: 0.2572\n",
            "Epoch 24/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8911 - loss: 0.2628 - val_accuracy: 0.8916 - val_loss: 0.2313\n",
            "Epoch 25/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8903 - loss: 0.2616 - val_accuracy: 0.8753 - val_loss: 0.2562\n",
            "Epoch 26/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8933 - loss: 0.2656 - val_accuracy: 0.8732 - val_loss: 0.2526\n",
            "Epoch 27/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8865 - loss: 0.2670 - val_accuracy: 0.8834 - val_loss: 0.2437\n",
            "Epoch 28/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8931 - loss: 0.2591 - val_accuracy: 0.8875 - val_loss: 0.2425\n",
            "Epoch 29/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8959 - loss: 0.2480 - val_accuracy: 0.8834 - val_loss: 0.2412\n",
            "Epoch 30/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9000 - loss: 0.2551 - val_accuracy: 0.8814 - val_loss: 0.2390\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │           \u001b[38;5;34m3,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │             \u001b[38;5;34m784\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m176\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m2,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,549\u001b[0m (80.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,549</span> (80.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,849\u001b[0m (26.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,849</span> (26.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m13,700\u001b[0m (53.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,700</span> (53.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy for sequence data: 0.9415\n",
            "Validation Accuracy for sequence data: 0.8814\n",
            "F1 Score for sequence data: 0.8802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_predictions_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjoy0RY-Q_83",
        "outputId": "44a80a9e-cec4-4e0b-bcdb-99e3de2b8f7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combined Data**"
      ],
      "metadata": {
        "id": "PjJsDUZFQCXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# read emoticon dataset\n",
        "train_emoticon_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/train/train_emoticon.csv\")\n",
        "train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
        "train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
        "\n",
        "valid_emoticon_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/valid/valid_emoticon.csv\")\n",
        "valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()\n",
        "valid_emoticon_Y = valid_emoticon_df['label'].tolist()\n",
        "\n",
        "test_emoticon_X = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/test/test_emoticon.csv\")['input_emoticon'].tolist()\n",
        "\n",
        "# Convert list to NumPy array\n",
        "train_emoticon_X = np.array(train_emoticon_X)\n",
        "valid_emoticon_X = np.array(valid_emoticon_X)\n",
        "test_emoticon_X = np.array(test_emoticon_X)\n",
        "\n",
        "train_emoticon_Y = np.array(train_emoticon_Y)\n",
        "valid_emoticon_Y = np.array(valid_emoticon_Y)\n",
        "\n",
        "print(type(train_emoticon_X))\n",
        "print(type(train_emoticon_X[0]))\n",
        "\n",
        "# Concatenate all lists into one\n",
        "all_texts = ''.join(train_emoticon_X.flatten()) + ''.join(valid_emoticon_X.flatten()) + ''.join(test_emoticon_X.flatten())\n",
        "\n",
        "# Get unique characters\n",
        "unique_characters = np.unique(list(all_texts))\n",
        "print(unique_characters)\n",
        "# Create a mapping from characters to indices\n",
        "char_to_index = {char: idx for idx, char in enumerate(unique_characters)}\n",
        "print(unique_characters.shape)\n",
        "print(len(char_to_index))\n",
        "\n",
        "# Function to convert input emoticons to a 13x226 matrix\n",
        "def one_hot_count_matrix(emoticon, max_len=13, num_classes=226):\n",
        "    # Initialize a matrix of zeros\n",
        "    matrix = np.zeros((max_len, num_classes))\n",
        "    # Iterate through the characters in the emoticon\n",
        "    for i, char in enumerate(emoticon):\n",
        "        if char in char_to_index:  # Check if char exists in the mapping\n",
        "            matrix[i, char_to_index[char]] = 1  # One-hot encoding\n",
        "        else:\n",
        "            print(f\"Character '{char}' not found in char_to_index.\")  # Optional: log missing characters\n",
        "    return matrix.flatten()  # Flatten to a vector for SVM input\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_emoticon_X = np.array([one_hot_count_matrix(emoticon) for emoticon in train_emoticon_X])\n",
        "valid_emoticon_X = np.array([one_hot_count_matrix(emoticon) for emoticon in valid_emoticon_X])\n",
        "test_emoticon_X = np.array([one_hot_count_matrix(emoticon) for emoticon in test_emoticon_X])\n",
        "\n",
        "print(train_emoticon_X.shape)\n",
        "print(valid_emoticon_X.shape)\n",
        "print(test_emoticon_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ-0qIMFPz9V",
        "outputId": "2ad0cb0c-e6cc-4487-d73c-765b031230f6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.str_'>\n",
            "['😀' '😁' '😂' '😃' '😄' '😅' '😆' '😇' '😈' '😉' '😊' '😋' '😌' '😍' '😎' '😏' '😐' '😑'\n",
            " '😒' '😓' '😔' '😕' '😖' '😗' '😘' '😙' '😚' '😛' '😜' '😝' '😞' '😟' '😠' '😡' '😢' '😣'\n",
            " '😤' '😥' '😦' '😧' '😨' '😩' '😪' '😫' '😬' '😭' '😮' '😯' '😰' '😱' '😲' '😳' '😴' '😵'\n",
            " '😶' '😷' '😸' '😹' '😺' '😻' '😼' '😽' '😾' '😿' '🙀' '🙁' '🙂' '🙃' '🙄' '🙅' '🙆' '🙇'\n",
            " '🙈' '🙉' '🙊' '🙋' '🙌' '🙍' '🙎' '🙏' '🙐' '🙑' '🙒' '🙓' '🙔' '🙕' '🙖' '🙗' '🙘' '🙙'\n",
            " '🙚' '🙛' '🙜' '🙝' '🙞' '🙟' '🙠' '🙡' '🙢' '🙣' '🙤' '🙥' '🙦' '🙧' '🙨' '🙩' '🙪' '🙫'\n",
            " '🙬' '🙭' '🙮' '🙯' '🙰' '🙱' '🙲' '🙳' '🙴' '🙵' '🙶' '🙷' '🙸' '🙹' '🙺' '🙻' '🙼' '🙽'\n",
            " '🙾' '🙿' '🚀' '🚁' '🚂' '🚃' '🚄' '🚅' '🚆' '🚇' '🚈' '🚉' '🚊' '🚋' '🚌' '🚍' '🚎' '🚏'\n",
            " '🚐' '🚑' '🚒' '🚓' '🚔' '🚕' '🚖' '🚗' '🚘' '🚙' '🚚' '🚛' '🚜' '🚝' '🚞' '🚟' '🚠' '🚡'\n",
            " '🚢' '🚣' '🚤' '🚥' '🚦' '🚧' '🚨' '🚩' '🚪' '🚫' '🚬' '🚭' '🚮' '🚯' '🚰' '🚱' '🚲' '🚳'\n",
            " '🚴' '🚵' '🚶' '🚷' '🚸' '🚹' '🚺' '🚻' '🚼' '🚽' '🚾' '🚿' '🛀' '🛁' '🛂' '🛃' '🛄' '🛅'\n",
            " '🛆' '🛇' '🛈' '🛉' '🛊' '🛋' '🛌' '🛍' '🛎' '🛏' '🛐' '🛑' '🛒' '🛓' '🛔' '🛕' '🛖' '🛗'\n",
            " '\\U0001f6d8' '\\U0001f6d9' '\\U0001f6da' '\\U0001f6db' '\\U0001f6dc'\n",
            " '\\U0001f6dd' '\\U0001f6de' '\\U0001f6df' '🛠' '🛡']\n",
            "(226,)\n",
            "226\n",
            "(7080, 2938)\n",
            "(489, 2938)\n",
            "(2232, 2938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the raw .npz file\n",
        "url_train = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/train/train_feature.npz\"\n",
        "url_valid = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/valid/valid_feature.npz\"\n",
        "url_test = \"https://github.com/psarath22/CS771_M1/raw/main/datasets/test/test_feature.npz\"\n",
        "# Download the file\n",
        "response1 = requests.get(url_train)\n",
        "response2= requests.get(url_valid)\n",
        "response3 = requests.get(url_test)\n",
        "\n",
        "# Save the content to a file\n",
        "with open('train_feature.npz', 'wb') as f:\n",
        "    f.write(response1.content)\n",
        "    print(\"Downloading TrainData completed!\")\n",
        "\n",
        "# Save the content to a file\n",
        "with open('valid_feature.npz', 'wb') as f:\n",
        "    f.write(response2.content)\n",
        "    print(\"Downloading validData completed!\")\n",
        "\n",
        "# Save the content to a file\n",
        "with open('test_feature.npz', 'wb') as f:\n",
        "    f.write(response3.content)\n",
        "    print(\"Downloading TestData completed!\")\n",
        "\n",
        "\n",
        "# read feat dataset\n",
        "train_feat = np.load(\"train_feature.npz\", allow_pickle=True)\n",
        "train_feat_X = train_feat['features']\n",
        "train_feat_Y = train_feat['label']\n",
        "\n",
        "test_feat_X = np.load(\"test_feature.npz\", allow_pickle=True)['features']\n",
        "\n",
        "valid_feat = np.load(\"valid_feature.npz\", allow_pickle=True)\n",
        "valid_feat_X = valid_feat['features']\n",
        "valid_feat_Y = valid_feat['label']\n",
        "\n",
        "\n",
        "# Create the training and validation  dataset\n",
        "\n",
        "train_feat_X = np.array([feat for feat in train_feat_X])\n",
        "valid_feat_X = np.array([feat for feat in valid_feat_X])\n",
        "test_feat_X = np.array([feat for feat in test_feat_X])\n",
        "\n",
        "train_feat_X = np.array([feat.flatten() for feat in train_feat_X])\n",
        "valid_feat_X = np.array([feat.flatten() for feat in valid_feat_X])\n",
        "test_feat_X = np.array([feat.flatten() for feat in test_feat_X])\n",
        "\n",
        "n_components = 150\n",
        "pca = PCA(n_components=n_components)\n",
        "train_feat_X = pca.fit_transform(train_feat_X)\n",
        "valid_feat_X = pca.transform(valid_feat_X)\n",
        "test_feat_X = pca.transform(test_feat_X)\n",
        "print(train_feat_X.shape)\n",
        "print(valid_feat_X.shape)\n",
        "print(test_feat_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNkuBwwUT1uE",
        "outputId": "10306f8a-5855-4a03-c23b-4691d26f9303"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TrainData completed!\n",
            "Downloading validData completed!\n",
            "Downloading TestData completed!\n",
            "(7080, 150)\n",
            "(489, 150)\n",
            "(2232, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read text sequence dataset\n",
        "train_seq_df = pd.read_csv('https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/train/train_text_seq.csv')\n",
        "train_seq_X = train_seq_df['input_str'].tolist()\n",
        "train_seq_Y = train_seq_df['label'].tolist()\n",
        "\n",
        "test_seq_X = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/test/test_text_seq.csv\")['input_str'].tolist()\n",
        "\n",
        "# read text sequence dataset\n",
        "valid_seq_df = pd.read_csv(\"https://raw.githubusercontent.com/psarath22/CS771_M1/main/datasets/valid/valid_text_seq.csv\")\n",
        "valid_seq_X = valid_seq_df['input_str'].tolist()\n",
        "valid_seq_Y = valid_seq_df['label'].tolist()\n",
        "\n",
        "\n",
        "# Convert list to NumPy array\n",
        "train_seq_X = np.array(train_seq_X)\n",
        "valid_seq_X = np.array(valid_seq_X)\n",
        "test_seq_X = np.array(test_seq_X)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "train_seq_Y = np.array(train_seq_Y)\n",
        "valid_seq_Y = np.array(valid_seq_Y)\n",
        "\n",
        "# Get unique characters for two digits\n",
        "unique_characters = [f\"{i:02d}\" for i in range(100)]  # ['00', '01', ..., '99']\n",
        "num_classes = len(unique_characters)  # 100 classes\n",
        "\n",
        "# Create a mapping from characters to indices\n",
        "char_to_index = {char: idx for idx, char in enumerate(unique_characters)}\n",
        "\n",
        "# Function to convert input seqs to a 49x100 matrix using overlapping pairs\n",
        "def one_hot_count_matrix(seq, max_len=50):\n",
        "    # Initialize a matrix of zeros\n",
        "    matrix = np.zeros((max_len - 1, num_classes))  # 49x100 matrix for overlapping pairs\n",
        "    # Extract overlapping pairs and update the matrix\n",
        "    for i in range(0, max_len - 1):  # Loop from seq[0:2] to seq[48:50]\n",
        "        char_pair = seq[i:i + 2]  # Get overlapping pairs of characters\n",
        "        if char_pair in char_to_index:\n",
        "            index = char_to_index[char_pair]\n",
        "            matrix[i, index] = 1  # One-hot encoding\n",
        "    return matrix.flatten()  # Flatten to a vector for SVM input\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_seq_X = np.array([one_hot_count_matrix(seq) for seq in train_seq_X])\n",
        "valid_seq_X = np.array([one_hot_count_matrix(seq) for seq in valid_seq_X])\n",
        "test_seq_X = np.array([one_hot_count_matrix(seq) for seq in test_seq_X])\n",
        "\n",
        "print(train_seq_X.shape)  # Expected shape: (N_train, 4900)\n",
        "print(valid_seq_X.shape)  # Expected shape: (N_valid, 4900)\n",
        "print(test_seq_X.shape)  # Expected shape: (N_test, 4900)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juJtz-ofT1wh",
        "outputId": "51a5ba33-2002-415e-faf9-a56a6156df23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7080, 4900)\n",
            "(489, 4900)\n",
            "(2232, 4900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined_X = np.concatenate((train_emoticon_X, train_feat_X, train_seq_X), axis=1)\n",
        "valid_combined_X = np.concatenate((valid_emoticon_X, valid_feat_X, valid_seq_X), axis=1)\n",
        "test_combined_X = np.concatenate((test_emoticon_X, test_feat_X, test_seq_X), axis=1)\n",
        "\n",
        "print(train_combined_X.shape)\n",
        "print(valid_combined_X.shape)\n",
        "print(test_combined_X.shape)\n",
        "\n",
        "train_combined_Y = train_emoticon_Y\n",
        "valid_combined_Y = valid_emoticon_Y\n",
        "\n",
        "num_features = 2938 + 150 + 4900\n",
        "print(f\"no.of features : {num_features}\")\n",
        "\n",
        "n_components = 300\n",
        "pca = PCA(n_components=n_components)\n",
        "train_combined_X = pca.fit_transform(train_combined_X)\n",
        "valid_combined_X = pca.transform(valid_combined_X)\n",
        "test_combined_X = pca.transform(test_combined_X)\n",
        "print(train_combined_X.shape)\n",
        "print(valid_combined_X.shape)\n",
        "print(test_combined_X.shape)\n",
        "\n",
        "print(f\"reduced no.of features : {n_components}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij6YcBlmT10A",
        "outputId": "c3a16a39-8720-4812-f040-05cac11c06b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7080, 7988)\n",
            "(489, 7988)\n",
            "(2232, 7988)\n",
            "no.of features : 7988\n",
            "(7080, 300)\n",
            "(489, 300)\n",
            "(2232, 300)\n",
            "reduced no.of features : 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_combined_XLogistic Regression classifier\n",
        "logreg_classifier = LogisticRegression(max_iter=1000)\n",
        "logreg_classifier.fit(train_combined_X, train_combined_Y)\n",
        "\n",
        "# Predict classes for validation data\n",
        "valid_predictions_combined = logreg_classifier.predict(valid_combined_X)\n",
        "train_predictions_combined = logreg_classifier.predict(train_combined_X)\n",
        "test_predictions_combined = logreg_classifier.predict(test_combined_X)\n",
        "\n",
        "valid_predictions_combined = valid_predictions_combined.flatten()\n",
        "train_predictions_combined = train_predictions_combined.flatten()\n",
        "test_predictions_combined = test_predictions_combined.flatten()\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_accuracy_combined = accuracy_score(train_combined_Y, train_predictions_combined)\n",
        "print(f\"Train Accuracy with Logistic Regression for Combined Data: {train_accuracy_combined}\")\n",
        "\n",
        "# Calculate validation accuracy\n",
        "valid_accuracy_combined = accuracy_score(valid_combined_Y, valid_predictions_combined)\n",
        "print(f\"Validation Accuracy with Logistic Regression for Combined Data: {valid_accuracy_combined}\")\n",
        "\n",
        "f1 = f1_score(valid_combined_Y, valid_predictions_combined, average='weighted')\n",
        "print(f\"F1 Score with Logistic Regression for Combined Data: {f1}\")\n",
        "\n",
        "# Calculate the number of trainable parameters\n",
        "n_weights = logreg_classifier.coef_.size  # Number of weights (features)\n",
        "n_intercepts = logreg_classifier.intercept_.size  # Number of intercepts (bias term)\n",
        "\n",
        "n_trainable_parameters = n_weights + n_intercepts\n",
        "print(f\"Number of trainable parameters for Combined Data: {n_trainable_parameters}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y6WChEDUTUD",
        "outputId": "2e86477d-392e-49ba-c70d-e40a1e398c69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy with Logistic Regression for Combined Data: 0.9879943502824858\n",
            "Validation Accuracy with Logistic Regression for Combined Data: 0.9815950920245399\n",
            "F1 Score with Logistic Regression for Combined Data: 0.9815938592004442\n",
            "Number of trainable parameters for Combined Data: 301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_predictions_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lkg_ggsV13H",
        "outputId": "b61f68d1-e595-4968-d0c2-f681463e2c8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 ... 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_to_file(predictions, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(f\"{pred}\\n\")# saving prediction to text files\n",
        "\n",
        "save_predictions_to_file(test_predictions_feat, \"pred_deepfeat.txt\")\n",
        "save_predictions_to_file(test_predictions_emoticon, \"pred_emoticon.txt\")\n",
        "save_predictions_to_file(test_predictions_seq, \"pred_textseq.txt\")\n",
        "save_predictions_to_file(test_predictions_combined, \"pred_combined.txt\")"
      ],
      "metadata": {
        "id": "nV6x7Lj0Wely"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFoe5SDZnOlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
